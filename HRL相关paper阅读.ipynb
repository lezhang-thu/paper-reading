{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HRL相关paper阅读.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data-Efficient Hierarchical Reinforcement Learning\n",
        "\n",
        "## Parameterized Rewards\n",
        "\n",
        "* 考虑Fig. 2中，从$g_0$到$g_{c-1}$，然后有说goal transition为：\n",
        "\n",
        "  \\begin{equation*}\n",
        "  h(s_t,g_t,s_{t+1})=s_t+g_t-s_{t+1}\n",
        "  \\end{equation*}\n",
        "\n",
        "  从这个角度，难道$g_{t+1}=h(s_t,g_t,s_{t+1})$，TODO\n",
        "\n",
        "  若成立，则会有：\n",
        "\n",
        "  \\begin{align*}\n",
        "  g_{t+1}&=s_t+g_t-s_{t+1}\\\\\n",
        "  g_{t+2}&=s_{t+1}+g_{t+1}-s_{t+2}\n",
        "  \\end{align*}\n",
        "\n",
        "  所以，telescope之后，会得到：$g_{t+2}=s_t+g_t-s_{t+2}$\n",
        "\n",
        "* 另一方面，从$r(s_t,g_t,a_t,s_{t+1})=-\\|s_t+g_t-s_{t+1}\\|_2$，所以，其是希望$\\|g_{t+1}\\|_2\\to 0$，TODO\n",
        "\n",
        "* That is, at step $t$, the higher-level policy produces a goal $g_t$, indicating its desire for the lower-level\n",
        "agent to take actions that yield it an observation $s_{t+c}$ that is close to $s_t + g_t$.\n",
        "\n",
        "  如果$g_{t+c}=s_t+g_t-s_{t+c}$，那么$\\|g_{t+c}\\|_2\\to 0$就实现了close to $s_t + g_t$的目标，TODO\n",
        "\n",
        "* DDPG的方式，针对lower-level policy\n",
        "    * 原本的形式：\n",
        "        * average Bellman error\n",
        "\n",
        "          \\begin{equation*}\n",
        "          \\bigl(Q_\\theta(s_t,a_t)-R_t-\\gamma Q_\\theta(s_{t+1},\\mu_\\phi(s_{t+1})\\bigr)^2\n",
        "          \\end{equation*}\n",
        "        \n",
        "        * 而$\\mu_\\phi$需maximize $Q_\\theta(s_t,\\mu_\\phi(s_t))$\n",
        "    * 新的形式：\n",
        "        * simply incorporating gt as an\n",
        "additional input into the value and policy models\n",
        "        * $Q^{lo}_\\theta(s_t,g_t,a_t)$\n",
        "        \n",
        "        * policy\n",
        "\n",
        "          \\begin{equation*}\n",
        "          \\mu^{lo}_\\phi(s_{t+1},g_{t+1})\n",
        "          \\end{equation*}\n",
        "\n",
        "## Off-Policy Corrections for Higher-Level Training\n",
        "\n",
        "* lower-level policy所拥有的信息：\n",
        "\n",
        "  \\begin{equation*}\n",
        "  (s_{t:t+c-1},g_{t:t+c-1},a_{t:t+c-1},R_{t:t+c-1},s_{t+c})\n",
        "  \\end{equation*}\n",
        "\n",
        "  需要将这些转换为higher-level policy能用的信息：\n",
        "\n",
        "  \\begin{equation*}\n",
        "  (s_t,g_t,\\sum R_{t:t+c-1},s_{t+c})\n",
        "  \\end{equation*}\n",
        "\n",
        "  注意$x_{t:t+c-1}$只是表示sequence，所以需要$\\sum$\n",
        "\n",
        "  $g_t$是higher-level policy的action\n",
        "\n",
        "* goal transition时用的$h$是fixed，即使是在correction的环节，所以，$g_{t:t+c-1}$中只有$g_t$是可以操作的\n",
        "\n",
        "* 该环节是对$g_t$进行re-label，得到$\\tilde{g}_t$\n",
        "\n",
        "* 关注的是\n",
        "\n",
        "  \\begin{equation*}\n",
        "  \\mu^{lo}(a_{t:t+c-1}|s_{t:t+c-1},\\tilde{g}_{t:t+c-1})\n",
        "  \\end{equation*}\n",
        "\n",
        "  如何去maximize，可以操作的实际仅为$\\tilde{g}_t$\n",
        "\n",
        "  * 其的做法：\n",
        "    * $\\arg\\max$上述的目标，相当于如下的目标\n",
        "      \n",
        "      \\begin{equation*}\n",
        "      -\\frac{1}{2}\\sum_{i=t}^{t+c-1}\\|a_i-\\mu^{lo}(s_i;\\tilde{g}_i)\\|^2_2\n",
        "      \\end{equation*}\n",
        "    \n",
        "    * 如何approximately solve\n",
        "\n",
        "      10个candidates"
      ],
      "metadata": {
        "id": "sgt7_KcSqbp0"
      }
    }
  ]
}